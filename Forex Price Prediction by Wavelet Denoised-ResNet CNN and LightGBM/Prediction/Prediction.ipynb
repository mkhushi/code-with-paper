{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MMMMMMMMM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76za-PNWL63T",
        "colab_type": "text"
      },
      "source": [
        "This code use all the dataset USDJPY M5 dataset from -2004.1.1-2020.6.9,if you want\n",
        "to change it,just slice the dataframe in the data processing stage.\n",
        "Meanwhile,in order to prevent the label is too small to affect the prediction, label has been multiplied by 1000 in data process stage.So when the final result is calculated, the value of the label will be divided by 1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAnTrsl5B2H5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pylab\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms \n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
        "import torchvision.transforms.functional as F\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from sklearn.metrics import mean_absolute_error "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVPeRoRlMm_t",
        "colab_type": "text"
      },
      "source": [
        "Get processed dataset from the google drive,or you can upload the file directly to the colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_z_EzHBD8aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "id = \"Use the dataset from DataProcessing\"\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('DeLabeledUSDJPY-5M-2019.1.1-2020.6.9.csv')\n",
        "df_JPYUSDM5=pd.read_csv('DeLabeledUSDJPY-5M-2019.1.1-2020.6.9.csv')\n",
        "\n",
        "with pd.option_context('display.precision', 20):  \n",
        "    dataM1 = pd.DataFrame(df_M1)\n",
        "\n",
        "    dataM5=pd.DataFrame(df_JPYUSDM5)\n",
        "    # show all the columns\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    # show all the rows\n",
        "    pd.set_option('display.max_rows', None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aBc4Jq7E8Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hypterparameters\n",
        "days=30\n",
        "BATCH_SIZE=64\n",
        "LR=1e-3\n",
        "epochs=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOUWYafbNDL5",
        "colab_type": "text"
      },
      "source": [
        "Defining functions and data processing for feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4jOgEz2FDjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TensorsDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    '''\n",
        "    A simple loading dataset - loads the tensor that are passed in input. This is the same as\n",
        "    torch.utils.data.TensorDataset except that you can add transformations to your data and target tensor.\n",
        "    Target tensor can also be None, in which case it is not returned.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, data_tensor, target_tensor=None, transforms=None, target_transforms=None):\n",
        "        if target_tensor is not None:\n",
        "            assert data_tensor.size(0) == target_tensor.size(0)\n",
        "        self.data_tensor = data_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "\n",
        "        if transforms is None:\n",
        "            transforms = []\n",
        "        if target_transforms is None:\n",
        "            target_transforms = []\n",
        "\n",
        "        if not isinstance(transforms, list):\n",
        "            transforms = [transforms]\n",
        "        if not isinstance(target_transforms, list):\n",
        "            target_transforms = [target_transforms]\n",
        "\n",
        "        self.transforms = transforms\n",
        "        self.target_transforms = target_transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        data_tensor = self.data_tensor[index]\n",
        "        for transform in self.transforms:\n",
        "            data_tensor = transform(data_tensor)\n",
        "\n",
        "        if self.target_tensor is None:\n",
        "            return data_tensor\n",
        "\n",
        "        target_tensor = self.target_tensor[index]\n",
        "        for transform in self.target_transforms:\n",
        "            target_tensor = transform(target_tensor)\n",
        "\n",
        "        return data_tensor, target_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_tensor.size(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oAOqdbGFJi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_absolute_percentage_error(y_pred, y_true):\n",
        "    mask=y_true!=0\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / (y_true[mask]))) * 100\n",
        "\n",
        "def normalize_function(data):\n",
        "    min = np.amin(data,axis=0)\n",
        "    max = np.amax(data,axis=0)\n",
        "    return (data - min)/(max-min)\n",
        "def labeled(x):\n",
        "  if x>=0:\n",
        "    return 0\n",
        "  if x<=-0:\n",
        "    return 1  \n",
        "\n",
        "def DA(predictions,y_test):\n",
        "    result=predictions*y_test\n",
        "    result=[1 if item>=0 else 0 for item in result]\n",
        "    return np.mean(result)\n",
        "\n",
        "def DA1(true,predict,yesday):\n",
        "    result=(true-yesday)*(predict-yesday)\n",
        "    result=[1 if item>=0 else 0 for item in result]\n",
        "    return np.mean(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL64_vQXFQMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IndicatorValueM5=dataM5[['Close_wave','Open_wave','High_wave','Low_wave','RSI5','RSI10','RSI20','macd','macd','macdsignal','slowk','slowd','fastk','fastd','WR5','WR10', 'WR20','ROC5','ROC10','ROC20','CCI5','CCI10','CCI20','ATR5','ATR10','ATR20','NATR5','NATR10','NATR20','TRANGE']].to_numpy()\n",
        "IndicatorValueM5=IndicatorValueM5[33:]\n",
        "# nomalize\n",
        "IndicatorValueM5=normalize_function(IndicatorValueM5)[:-4]\n",
        "print(IndicatorValueM5.shape)\n",
        "IndicatorValueM5=np.array([IndicatorValueM5[i-days:i] for i in range(days,len(IndicatorValueM5))])\n",
        "print(IndicatorValueM5.shape)\n",
        "\n",
        "labelM5=dataM5.label4[days:].to_numpy()\n",
        "labelM5=labelM5[33:-4]\n",
        "print(labelM5.shape)\n",
        "\n",
        "\n",
        "\n",
        "IndicatorValueM5=torch.tensor(IndicatorValueM5)\n",
        "IndicatorValueM5 = torch.unsqueeze(IndicatorValueM5, 1)\n",
        "labelM5=torch.tensor(labelM5)\n",
        "print(IndicatorValueM5.shape)\n",
        "\n",
        "# Cureently FinalM5_train use all the dataset to extract the feature,if you want to train, test or verification the model \n",
        "# remove the following annotations or change it to any length you want\n",
        "\n",
        "# FinalM5_train=IndicatorValueM5[:99000]\n",
        "FinalM5_train=IndicatorValueM5\n",
        "FinalM5_test=IndicatorValueM5[99000:]\n",
        "\n",
        "# labelM5_train=labelM5[:99000]\n",
        "labelM5_train=labelM5\n",
        "labelM5_test=labelM5[99000:]\n",
        "\n",
        "transformer = transforms.Compose([\n",
        "  # transforms.Resize((224,224)),\n",
        "  # transforms.ToPILImage() ,\n",
        "  # transforms.ToTensor(), \n",
        "   \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtNp2994GSk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FinaldatasetM5_train = TensorsDataset(FinalM5_train, labelM5_train,transformer)\n",
        "FinaldatasetM5_test = TensorsDataset(FinalM5_test, labelM5_test,transformer)\n",
        "FinaldatasetM5_FE=TensorsDataset(IndicatorValueM5, labelM5,transformer)\n",
        "\n",
        "data_loader_FinalM5train = DataLoader(FinaldatasetM5_train, batch_size=BATCH_SIZE,shuffle=True)\n",
        "data_loader_FinalM5test = DataLoader(FinaldatasetM5_test, batch_size=BATCH_SIZE,shuffle=False)\n",
        "data_loader_FinalM5FE = DataLoader(FinaldatasetM5_FE, batch_size=BATCH_SIZE,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-_SDrAbNeWT",
        "colab_type": "text"
      },
      "source": [
        "Define ResNet structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Snn24VHSN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = torch.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=1024):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.conv0 = nn.Sequential(         \n",
        "            nn.Conv2d(1, 3, 1,1,0), \n",
        "            nn.BatchNorm2d(3,affine=True), \n",
        "              nn.ELU(),                      \n",
        "        )\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(1024, momentum=0.9)\n",
        "        self.linear2= nn.Linear(1024,256)\n",
        "        self.bn2 = nn.BatchNorm1d(256, momentum=0.9)\n",
        "        self.linear3= nn.Linear(256,1)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = torch.nn.functional.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        output=self.bn(out)\n",
        "        output = torch.relu(output)\n",
        "        output=self.linear2(output)\n",
        "        output=self.bn2(output)\n",
        "        output = torch.relu(output)\n",
        "        output=self.linear3(output)\n",
        "        return output,out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35qdHlqtHfVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(1, 3, 1,1,0), \n",
        "            nn.BatchNorm2d(3,affine=True), \n",
        "              nn.ELU(),                      \n",
        "        )\n",
        "        self.net= models.resnext50_32x4d(pretrained=False)\n",
        "        # self.net=models.wide_resnet50_2(pretrained=True)\n",
        "       \n",
        "        \n",
        "        self.net._modules[\"fc\"] = nn.Linear(2048,1024)      \n",
        "        self.linear1 = nn.Linear(1024,1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024, momentum=0.9)\n",
        "        self.linear2= nn.Linear(1024,256)\n",
        "        self.bn2 = nn.BatchNorm1d(256, momentum=0.9)\n",
        "        self.linear3= nn.Linear(256,1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "            x = self.conv1(x)        \n",
        "            x = self.net(x)\n",
        "            x = self.linear1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = torch.relu(x)\n",
        "            output=self.linear2(x)\n",
        "            output=self.bn2(output)\n",
        "            output = torch.relu(output)\n",
        "            output=self.linear3(output)\n",
        "            \n",
        "  \n",
        "            return output,x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4z-lRiBH443",
        "colab_type": "text"
      },
      "source": [
        "Currently,this code use all the dataset to extract the feature,if you want to retrain the ResNet using the train,test dataset,remove the following test and verification annotations(Don't forget to change the current train dataset and code,it depends on how large datasets you want to use and how to allocate train test sets).These codes will also save the best model automatically and extract the features in the next part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS0N0QWQHl22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hypterparameters\n",
        "days=30\n",
        "BATCH_SIZE=32\n",
        "LR=1e-3\n",
        "epochs=100\n",
        "\n",
        "# Mynet=ResNet(Bottleneck, [3,4,6,3])\n",
        "Mynet=MyModel()\n",
        "Mynet = Mynet.double()\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "if use_gpu:\n",
        "    print(\"using gpu\")\n",
        "    Mynet = Mynet.cuda()\n",
        "optimizer = torch.optim.Adam(Mynet.parameters(), lr=LR)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "temp_loss=100000\n",
        "for epoch in range(epochs):\n",
        "  #Training phase\n",
        "  total_training_loss = 0.0\n",
        "  i=0\n",
        "  pre=torch.zeros(1,1).double()\n",
        "  label=torch.zeros(1,1).double()\n",
        "  ###\n",
        "  feature=torch.zeros(1,1024).double()\n",
        "  ###\n",
        "  for iter, traindata in enumerate(data_loader_FinalM5train):\n",
        "    i+=1\n",
        "    train_inputs, train_labels = traindata\n",
        "   \n",
        "    \n",
        "    if use_gpu:    \n",
        "      train_inputs, train_labels = train_inputs.cuda(), train_labels.cuda()      \n",
        "    else:\n",
        "      train_inputs, train_labels = train_inputs, train_labels     \n",
        "   \n",
        "\n",
        "    train_labels=train_labels.view(-1,1)\n",
        "    # train_labels=torch.unsqueeze(train_labels, 1)\n",
        "\n",
        "\n",
        "    train_outputs,feature1 = Mynet(train_inputs.double())\n",
        "    \n",
        "\n",
        "    loss = criterion(train_outputs, train_labels)\n",
        "    optimizer.zero_grad()\n",
        "    # torch.set_default_tensor_type(torch.FloatTensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "      \n",
        "    total_training_loss += loss.data\n",
        "\n",
        "    pre=torch.cat((pre.cuda(),train_outputs.cuda()), 0)\n",
        "    label=torch.cat((label.cuda(),train_labels.cuda()), 0)\n",
        "    feature=torch.cat((feature.cuda(),feature1.cuda()), 0)\n",
        "\n",
        "\n",
        "    MSE=mean_squared_error(train_outputs.cpu().detach().numpy()/1000,train_labels.cpu().detach().numpy()/1000)\n",
        "    MAE=mean_absolute_error(train_outputs.cpu().detach().numpy()/1000,train_labels.cpu().detach().numpy()/1000)\n",
        "    RMSE=MSE ** 0.5\n",
        "    MAPE=mean_absolute_percentage_error(train_outputs.cpu().detach().numpy()/1000,train_labels.cpu().detach().numpy()/1000)\n",
        "\n",
        "    # print(\"OUTPUT\",train_outputs[:10])\n",
        "    # print(\"LABEL\",train_labels[:10])\n",
        "    if (iter + 1) %100 == 0:\n",
        "      print('Training Phase: Epoch: [%2d][%2d/%2d]\\tIteration Loss: %.6f\\t' %\n",
        "                (iter+1, epoch+1, epochs, loss.data ))#loss.data / train_labels.size(0)\n",
        "      print('MAE %.6f*10-3\\t MSE: %.6f*10-3\\tRMSE: %.6f*10-3\\tMAPE: %.6f%%\\t' %\n",
        "                (1000*MAE,1000*MSE,1000*RMSE,MAPE ))\n",
        "  print(\"EPOCH: %2d\\t Total Training Loss：%.6f\\t \"%\n",
        "                ( epoch+1, total_training_loss/i))\n",
        "  pre=pre[1:]\n",
        "  label=label[1:] \n",
        "  feature=feature.cpu().detach().numpy()\n",
        "  feature=feature[1:]\n",
        "  MSE=mean_squared_error(pre.cpu().detach().numpy()/1000,label.cpu().detach().numpy()/1000)\n",
        "  MAE=mean_absolute_error(pre.cpu().detach().numpy()/1000,label.cpu().detach().numpy()/1000)\n",
        "  RMSE=MSE ** 0.5\n",
        "  MAPE=mean_absolute_percentage_error(pre.cpu().detach().numpy()/1000,label.cpu().detach().numpy()/1000)\n",
        "  print('TOTAL TRAIN:MAE %.6f*10-3\\t MSE: %.6f*10-3\\tRMSE: %.6f*10-3\\tMAPE: %.6f%%\\t' %\n",
        "                (1000*MAE,1000*MSE,1000*RMSE,MAPE ))\n",
        "  # print(\"PREDICTION:\",pre[:10])\n",
        "  # print(\"LABEL:\",label[:10])\n",
        "\n",
        "  ####\n",
        "  # torch.save(Mynet, 'Mynet.pkl')\n",
        "  ####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #TEST Phase\n",
        "#   i=0\n",
        "#   total_test_loss=0.0\n",
        "#   pre=torch.zeros(1,1).double()\n",
        "#   label=torch.zeros(1,1).double()\n",
        "#   for iter, testdata in enumerate(data_loader_FinalM5test):\n",
        "#     i+=1\n",
        "#     test_inputs, test_labels = testdata\n",
        "#     if use_gpu:\n",
        "#       test_inputs, test_labels = test_inputs.cuda(), test_labels.cuda()\n",
        "#     else:\n",
        "#       test_inputs, test_labels = test_inputs, test_labels\n",
        "#     test_labels=test_labels.view(-1,1)\n",
        "#     test_outputs = Mynet(test_inputs.double())[0]\n",
        "#     test_loss = criterion(test_outputs, test_labels)\n",
        "#     total_test_loss += test_loss.data\n",
        "#     pre=torch.cat((pre.cuda(),test_outputs.cuda()), 0)\n",
        "#     label=torch.cat((label.cuda(),test_labels.cuda()), 0)\n",
        "#   print(\"EPOCH: %2d\\tTotal TEST Loss：%.6f\\t \"%\n",
        "#                 (epoch+1,total_test_loss/i))\n",
        "#   pre=pre[1:]\n",
        "#   label=label[1:]\n",
        "#   acc=DA(pre.cpu().detach().numpy(),label.cpu().detach().numpy())\n",
        "#   MSE=mean_squared_error(pre.cpu().detach().numpy()/1000,label.cpu().detach().numpy()/1000)\n",
        "#   MAE=mean_absolute_error(pre.cpu().detach().numpy()/1000,label.cpu().detach().numpy()/1000)\n",
        "#   RMSE=MSE ** 0.5\n",
        "#   MAPE=mean_absolute_percentage_error(pre.cpu().detach().numpy()/1000,label.cpu().detach().numpy()/1000)\n",
        "#   print('TOTAL TEST:MAE %.6f*10-3\\t MSE: %.6f*10-3\\tRMSE: %.6f*10-3\\tMAPE: %.6f%%\\tDA: %.6f%%\\t' %\n",
        "#                 (1000*MAE,1000*MSE,1000*RMSE,MAPE,acc ))\n",
        "#   print(\"PREDICTION:\",pre[:10])\n",
        "#   print(\"LABEL:\",label[:10])\n",
        "  \n",
        "#   print(\"########################################\\n########################################\")\n",
        "#   testloss=total_test_loss/i\n",
        "#   if testloss<temp_loss:\n",
        "#     temp_loss=testloss\n",
        "#     lowest_epo=epoch\n",
        "#     lowest_MAE=MAE\n",
        "#     lowest_MSE=MSE\n",
        "#     lowest_RMSE=RMSE\n",
        "#     lowest_MAPE=MAPE\n",
        "#     lowest_acc=acc\n",
        "#     torch.save(Mynet, 'Mynet.pkl') \n",
        "\n",
        "# print(\"LOWEST TEST LOSS:%.6f\\t in the epoch:%2d\\t\"%(temp_loss,lowest_epo+1))\n",
        "# print('In this epoch :MAE %.6f*10-3\\t MSE: %.6f*10-3\\tRMSE: %.6f*10-3\\tMAPE: %.6f%%\\tACC: %.6f%%\\t' %\n",
        "#                 (1000*lowest_MAE,1000*lowest_MSE,1000*lowest_RMSE,lowest_MAPE,lowest_acc ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtJQIkTfJie8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # recover from the stored network\n",
        "\n",
        "# # verification\n",
        "\n",
        "\n",
        "\n",
        "# use_gpu = torch.cuda.is_available()\n",
        "# recoverNet = torch.load('Mynet.pkl') \n",
        "# total_test_loss=0.0\n",
        "# pre=torch.zeros(1,1).double()\n",
        "# label=torch.zeros(1,1).double()\n",
        "# criterion = nn.MSELoss()\n",
        "# i=0\n",
        "# for iter, testdata in enumerate(data_loader_FinalM5test):\n",
        "#   i+=1\n",
        "#   test_inputs, test_labels = testdata\n",
        "#   if use_gpu:\n",
        "#     test_inputs, test_labels = test_inputs.cuda(), test_labels.cuda()\n",
        "#   else:\n",
        "#     test_inputs, test_labels = test_inputs, test_labels\n",
        "#   test_labels=test_labels.view(-1,1)\n",
        "#   test_outputs = recoverNet(test_inputs.double())[0]\n",
        "#   test_loss = criterion(test_outputs, test_labels)\n",
        "#   total_test_loss += test_loss.data\n",
        "#   pre=torch.cat((pre.cuda(),test_outputs.cuda()), 0)\n",
        "#   label=torch.cat((label.cuda(),test_labels.cuda()), 0)\n",
        "# print(\"Recover Loss：%.6f\\t \"%\n",
        "#                 (total_test_loss/i))\n",
        "# pre=pre[1:]\n",
        "# label=label[1:] \n",
        "# MSE=mean_squared_error(pre.cpu().detach().numpy()/1000,label.cpu().detach().numpy()/1000)\n",
        "# MAE=mean_absolute_error(pre.cpu().detach().numpy()/1000,label.cpu().detach().numpy()/1000)\n",
        "# RMSE=MSE ** 0.5\n",
        "# MAPE=mean_absolute_percentage_error(pre.cpu().detach().numpy()/1000,label.cpu().detach().numpy()/1000)\n",
        "# acc=DA(pre.cpu().detach().numpy(),label.cpu().detach().numpy())\n",
        "# print('Recover TEST:MAE %.6f*10-3\\t MSE: %.6f*10-3\\tRMSE: %.6f*10-3\\tMAPE: %.6f%%\\DA: %.6f\\t' %\n",
        "#                 (1000*MAE,1000*MSE,1000*RMSE,MAPE,acc))\n",
        "\n",
        "\n",
        "# # ectract feature and get prediction\n",
        "# feature=torch.zeros(1,1024).double()\n",
        "# prediction=torch.zeros(1,1).double()\n",
        "# for iter, testdata in enumerate(data_loader_FinalM5FE):\n",
        "#   test_inputs, test_labels = testdata\n",
        "#   if use_gpu:\n",
        "#     test_inputs, test_labels = test_inputs.cuda(), test_labels.cuda()\n",
        "#   else:\n",
        "#     test_inputs, test_labels = test_inputs, test_labels\n",
        "#   test_labels=test_labels.view(-1,1)\n",
        "#   with torch.no_grad():\n",
        "#     torch.cuda.empty_cache()\n",
        "#     test_outputs,feature1 = recoverNet(test_inputs.double())\n",
        "#     feature=torch.cat((feature.cuda(),feature1.cuda()), 0)\n",
        "#     prediction=torch.cat((prediction.cuda(),test_outputs.cuda()), 0)\n",
        "\n",
        "# prediction=prediction[1:]\n",
        "# feature=feature[1:]\n",
        "# feature=feature.cpu().detach().numpy()\n",
        "# print(feature.shape)\n",
        "\n",
        "\n",
        "# prediction=prediction.cpu().detach().numpy()\n",
        "# print(prediction.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8A_IMXLNtWC",
        "colab_type": "text"
      },
      "source": [
        "Using LightGBM to deal with the extracted features from ResNet and the technical indicators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9stXJPN_Jz_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "# x=a.reshape((-1,days*a.shape[-1]))\n",
        "x1=feature\n",
        "print(x1.shape)\n",
        "x2=dataM5[['RSI5','RSI10','RSI20','macd','macd','macdsignal','slowk','slowd','fastk','fastd','WR5','WR10', 'WR20','ROC5','ROC10','ROC20','CCI5','CCI10','CCI20','ATR5','ATR10','ATR20','NATR5','NATR10','NATR20','TRANGE']].to_numpy()\n",
        "x2=x2[33:-4]\n",
        "x2=np.array([x2[i-days:i] for i in range(days,len(x2))])\n",
        "print(x2.shape)\n",
        "x2=x2.reshape((-1,days*x2.shape[-1]))\n",
        "print(x2.shape)\n",
        "x=np.concatenate([x2, x1], 1)\n",
        "\n",
        "y=dataM5.label4[33:].to_numpy()\n",
        "y=y[days:-4]\n",
        "\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=33) \n",
        "params = {'num_leaves': 38,              \n",
        "          'min_data_in_leaf': 50,\n",
        "          'objective': 'regression',     \n",
        "          'max_depth': -1,                \n",
        "          'learning_rate': 0.1,           \n",
        "          \"min_sum_hessian_in_leaf\": 6,\n",
        "          \"boosting\": \"gbdt\",             \n",
        "          \"feature_fraction\": 0.9,         \n",
        "          \"bagging_freq\": 1,               \n",
        "          \"bagging_fraction\": 0.7,          \n",
        "          \"bagging_seed\": 11,\n",
        "          \"lambda_l1\": 0.1,\n",
        "          \"verbosity\": -1,               \n",
        "          \"nthread\": 4,               \n",
        "          'metric': 'mae',               \n",
        "          \"random_state\": 2019,\n",
        "          # 'device': 'gpu'\n",
        "          }\n",
        "trn_data = lgb.Dataset(x_train, y_train)\n",
        "val_data = lgb.Dataset(x_test, y_test)\n",
        "\n",
        "clf = lgb.train(params,\n",
        "                trn_data,\n",
        "                20000,\n",
        "                valid_sets=[trn_data, val_data],\n",
        "                verbose_eval=200,\n",
        "                early_stopping_rounds=500)\n",
        "oof = clf.predict(x_train, num_iteration=clf.best_iteration)\n",
        "predictions = clf.predict(x_test, num_iteration=clf.best_iteration)\n",
        "\n",
        "\n",
        "MSE=mean_squared_error(predictions/1000,y_test/1000)\n",
        "MAE=mean_absolute_error(predictions/1000,y_test/1000)\n",
        "RMSE=MSE ** 0.5\n",
        "MAPE=mean_absolute_percentage_error(predictions/1000,y_test/1000)\n",
        "acc=DA(predictions,y_test)\n",
        "\n",
        "print('MAE %.6f*10-3\\t MSE: %.6f*10-3\\tRMSE: %.6f*10-3\\tMAPE: %.6f%%\\tACC: %.6f' %\n",
        "                (1000*MAE,1000*MSE,1000*RMSE,MAPE,acc ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfYkOMY-KVJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions1 = clf.predict(x, num_iteration=clf.best_iteration)\n",
        "dataM5['pre_Value'] = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBqfQVBJOCaO",
        "colab_type": "text"
      },
      "source": [
        "Save the results to the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65jS17-UKYAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,predictions1.shape[0]):\n",
        "    dataM5['pre_Value'][i+63]=predictions1[i]\n",
        "\n",
        "dataM5.to_csv('D1_Predicted.csv')\n",
        "# Save file to google drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "# Create & upload a text file.\n",
        "uploaded = drive.CreateFile({'title': 'D1_Predicted.csv'})\n",
        "uploaded.SetContentFile('D1_Predicted.csv')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}